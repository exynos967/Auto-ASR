# Auto-ASR（Gradio + 多后端 ASR + 字幕处理）

一个基于 Gradio 的ASR工具：上传/录制音频后进行转写，并导出字幕文件。支持多个转写后端，并提供一套可配置的字幕 LLM 处理流程（校正/翻译/智能断句）。

## 功能一览

- 转写后端
  - OpenAI 接口（远程）
  - FunASR（本地）
  - Transformers 后端（本地；当前默认用于 Qwen3-ASR）
- 长音频切分与字幕轴
  - 统一使用 **Silero VAD** 做切分/语音段时间轴
- 字幕处理（LLM）
  - 字幕校正
  - 字幕翻译
  - 智能断句

## 快速开始

1) 安装依赖

```bash
uv sync
```

2) 启动WebUI

```bash
uv run python app.py
```

启动后会打印本地访问地址（例如 `http://127.0.0.1:7860`），用浏览器打开即可。

3) 配置与持久化

- 网页界面中的配置会自动保存到项目根目录 `.auto_asr_config.json`
- 如需重置配置：删除 `.auto_asr_config.json`

## 可选：启用本地推理后端

### Qwen3-ASR（Transformers 本地推理）

安装：

```bash
uv sync --extra transformers
```

模型地址：

- Qwen3-ASR-1.7B：`https://huggingface.co/Qwen/Qwen3-ASR-1.7B`

使用方式：

- 在网页界面「引擎配置 -> Qwen3-ASR 本地推理」里点击「下载模型 / 加载模型」

### FunASR（本地推理）注:效果不如Qwen-ASR

安装：

```bash
uv sync --extra funasr
```

常用模型：

- SenseVoiceSmall：`https://huggingface.co/iic/SenseVoiceSmall`
- Fun-ASR-Nano-2512：`https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512`

## 切分与字幕轴（Silero VAD）

在网页界面「切分与字幕轴」里：

- `启用 VAD`：用于长音频切分、以及“语音段模式”的时间轴
- `时间轴策略`
  - `vad_speech`：优先按语音段输出字幕轴（推荐；对长音频更稳定）
  - `chunk`：按切分块输出字幕轴

### 参数含义与调参建议（与 WebUI 控件一一对应）

#### 1) 长音频切分

这块决定“整段音频要不要拆成多个 chunk 逐段转写”，主要影响：

- 长音频是否更稳定（避免一次性推理导致超限/爆显存）
- 总调用次数（chunk 越多调用越多）

可调参数：

- `启用 VAD（用于长音频切分 / 语音段模式）`
  - 含义：开启后会优先使用 Silero VAD 辅助切分；关闭时会尽量整段转写（或走固定时长分段的降级逻辑）。
  - 调大/调小：这是开关。
  - 建议：长音频建议开启；短音频或排障时可临时关闭。
- `目标分段时长（秒）`
  - 含义：切分时会以“目标时长”为基准，在接近目标位置的语音边界附近选一个切点，让每个 chunk 接近这个时长。
  - 调大：chunk 更长、调用更少；但单次推理更吃显存/更容易触发服务端大小限制（远程 ASR）或长音频 OOM（本地 ASR）。
  - 调小：chunk 更短、调用更多；更稳、更不容易 OOM，但速度可能下降（调用/上传/解码开销增加）。
- `最大分段时长（秒）`
  - 含义：硬上限。即使切分点不理想或 VAD 失败，也会确保 chunk 不超过这个长度（必要时会强制再切小）。
  - 调大：更少分段，但更容易出现单段太长导致 OOM/超限。
  - 调小：更多分段，更稳但调用更多。

说明（重要）：

- “长音频切分”不是纯按静音切段：它会先用 VAD 找到语音边界，再围绕“目标分段时长”去选切点。
- 因此你只调整「VAD 灵敏度」时，chunk 数有时可能变化不明显；chunk 数更多由「目标分段时长/最大分段时长」决定。

#### 2) VAD 灵敏度（影响“哪里算语音/静音”）

这块会同时影响：

- 长音频切分（VAD 找到的语音边界会影响切点选择）
- “按 VAD 语音段”时间轴（语音段边界是否贴合真实说话）

可调参数：

- `VAD 阈值（越低越敏感，越不容易漏语气词）`
  - 含义：把某一帧判断为“语音”的置信度阈值。
  - 调大：更“保守”，更少把噪音当语音；但更容易漏掉轻声/气声/语气词，可能导致字幕缺字或时间轴断裂。
  - 调小：更“敏感”，更不容易漏字；但可能把背景噪声也当语音，导致语音段变长、分段变少或字幕更“粘连”。
- `最小语音时长（ms）`
  - 含义：过滤太短的“语音片段”，避免把瞬时噪点当语音。
  - 调大：忽略更短的语音，语音段更少更干净；但短词/拟声词可能被丢掉。
  - 调小：更容易保留短词/语气词；但也更容易引入噪点，导致语音段碎片化。
- `最小静音时长（ms）`
  - 含义：判断“语音已结束/可以断开”的最短静音要求。
  - 调大：需要更长的停顿才会断开，语音段更长、段数更少（更省调用），但字幕更可能合并在一起。
  - 调小：短暂停顿就会断开，语音段更短、段数更多（更细的时间轴），但会增加调用次数。
- `语音段边缘填充（ms，避免切掉开头/结尾字）`
  - 含义：在每个语音段的左右两边额外“多留一点”时间，避免 VAD 边界过紧切掉字。
  - 调大：更不容易切掉字，但会引入更多前后静音/噪声，语音段也会变长（可能导致相邻语音段更容易合并）。
  - 调小：边界更紧凑，但更容易切掉开头/结尾字（尤其是爆破音、轻声）。

场景示例（建议起点；不同片源差异很大，按“是否漏字/是否把噪声当语音/段数是否过多”微调）：

- 电影/综艺（背景音乐、音效多，人声相对清晰）
  - 目标：减少把 BGM/音效当语音，同时别漏对白。
  - 建议：`VAD 阈值` 0.35–0.55、`最小语音时长` 150–300、`最小静音时长` 400–800、`边缘填充` 200–400
  - 现象 → 调参：
    - 背景音乐被当成语音：阈值调大、最小语音时长调大
    - 对白漏字/断裂：阈值调小、边缘填充调大
- ASMR/耳语（音量低、气声多、细碎噪点多）
  - 目标：尽量别漏掉轻声/耳语，同时避免气声/口水音导致段数爆炸。
  - 建议：`VAD 阈值` 0.15–0.30、`最小语音时长` 50–150、`最小静音时长` 300–600、`边缘填充` 400–800
  - 现象 → 调参：
    - 轻声经常漏掉：阈值调小、边缘填充调大
    - 段数太多（气声/呼吸也被当语音）：最小语音时长调大、最小静音时长调大

#### 3) 字幕轴（模型不支持字幕轴时）

当 ASR 后端本身不输出时间戳（或输出不稳定）时，本项目会用 Silero VAD 来“补”时间轴。

- `时间轴策略`
  - `vad_speech`：按 VAD 语音段生成时间轴。时间轴更贴合说话节奏，但语音段越碎，调用次数越多（尤其是远程 ASR）。
  - `chunk`：按“长音频切分”的 chunk 生成时间轴。调用更少，但时间轴更粗（通常一段字幕覆盖一大段音频）。
- `语音段最大时长（秒）`
  - 含义：在 `vad_speech` 模式下，单条语音段的硬上限；超过会被强制切小（避免单段太长）。
  - 调大：语音段更长、段数更少（更省调用）；但单段字幕更长，且本地推理更容易 OOM。
  - 调小：语音段更短、段数更多（时间轴更细）；但调用次数会上升。
- `合并相邻语音段的静音阈值（毫秒）`
  - 含义：把“相隔很近的两段语音”合并为一段（停顿小于该阈值就合并）。
  - 调大：更容易合并，语音段更少、更长（省调用），但更可能把两句话粘到一条字幕里。
  - 调小：更少合并，语音段更碎（更细时间轴），但调用更多。

补充说明：

- FunASR 内置 VAD 已移除；Qwen3-ASR 强制对齐模型已移除。
- 当前项目内所有“切分/字幕轴”统一走 Silero VAD（VAD 不可用时会自动降级为固定分段，确保流程可用）。

## 字幕处理

在网页界面「字幕处理」标签页：

- 支持上传 `.srt/.vtt` 后做：
  - 字幕校正（LLM）
  - 字幕翻译（LLM）
  - 智能断句（LLM）
- LLM 配置独立于“转写引擎配置”，在「LLM 提供商（仅字幕处理）」区域配置即可
- 输出目录：默认写入 `./outputs/processed/`

## 模型与缓存目录

- 模型与下载缓存统一放在项目目录 `./models/` 下
- 具体子目录结构取决于下载器：
  - ModelScope：通常在 `./models/hub/models/<组织>/<模型名>/...`
  - HuggingFace：通常在 `./models/huggingface/hub/...`

## 常见问题

- `No module named 'typer'`（Gradio 依赖缺失）：
  - 先执行 `uv sync`，若仍报错可尝试：`uv pip install -U typer`
- Windows + Python 3.12 安装 FunASR 相关依赖失败（`llvmlite/numba`）：
  - 建议使用 Python 3.11（或 3.10）创建环境再安装：

```bash
uv python install 3.11
uv sync --extra funasr -p 3.11
```

- CUDA 加速（本地推理）：
  - 可按你的 CUDA 版本安装对应的 PyTorch，例如（CUDA 12.8）：

```bash
uv pip install --upgrade --torch-backend cu128 torch torchaudio
```

- ffmpeg：
  - 项目使用 `imageio-ffmpeg` 提供 ffmpeg

## Gradio 主题

- MIKU：`https://huggingface.co/spaces/NoCrypt/miku`
